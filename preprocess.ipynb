{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-06 10:26:46.074236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-06 10:26:46.608196: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import rdkit\n",
    "from tqdm import tqdm\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder path\n",
    "DATASET_DIR = './datasets/'\n",
    "MAPPING_DIR = './mapping/'\n",
    "\n",
    "# SMILES path\n",
    "SMILES_FILENAME = '250k_rndm_zinc_drugs_clean' # do not include '.smi' extension\n",
    "\n",
    "# MAPPING TABLE PATH\n",
    "MAPPING_TABLE_DIR = MAPPING_DIR + SMILES_FILENAME\n",
    "WORD_IDX_FILENAME = 'word_to_idx.json'\n",
    "IDX_WORD_FILENAME = 'idx_to_word.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Directory alread exists\n"
     ]
    }
   ],
   "source": [
    "# Create mapping folder\n",
    "if not os.path.isdir(MAPPING_TABLE_DIR):\n",
    "    try:\n",
    "        os.makedirs(MAPPING_TABLE_DIR)\n",
    "    except OSError:\n",
    "        logging.error(\"Creation of the directory failed\")\n",
    "    else:\n",
    "        print(\"Successfully created the directory\")\n",
    "else:\n",
    "    logging.warning(\"Directory alread exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 31655.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of SMILES: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "# note: when char_tokenizerl=True, then filters not working! so we need split \\n\n",
    "tokenizer = text.Tokenizer(char_level=True, lower=False)\n",
    "\n",
    "# Load data\n",
    "with open(DATASET_DIR + SMILES_FILENAME + '.smi', 'r') as f:\n",
    "    for smiles in tqdm(f.readlines()):\n",
    "        smiles = smiles.split('\\n')[0]\n",
    "        tokenizer.fit_on_texts(smiles)\n",
    "        \n",
    "    word_index = tokenizer.word_index\n",
    "    print('Total number of SMILES: {}'.format(len(word_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1, 'c': 2, '<START>': 3, '<PAD>': 4, '<EOL>': 5}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add prefix\n",
    "SMILES_DIM = len(word_index)\n",
    "word_index['<START>'] = SMILES_DIM + 1 \n",
    "word_index['<PAD>'] = SMILES_DIM + 2\n",
    "word_index['<EOL>'] = SMILES_DIM + 3 \n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'C', 2: 'c', 3: '<START>', 4: '<PAD>', 5: '<EOL>'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index to word\n",
    "index_word = {index:word for word, index in word_index.items()}\n",
    "index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "## using tokenizer class method\n",
    "tokenizer_json = tokenizer.to_json()\n",
    "with open(MAPPING_TABLE_DIR + '/tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(tokenizer_json, ensure_ascii=False))\n",
    "\n",
    "## old school|\n",
    "with open(MAPPING_TABLE_DIR + '/' + WORD_IDX_FILENAME, 'w') as wi, \\\n",
    "    open(MAPPING_TABLE_DIR + '/' + IDX_WORD_FILENAME, 'w') as iw:\n",
    "        wi.write(json.dumps(word_index))\n",
    "        iw.write(json.dumps(index_word))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
